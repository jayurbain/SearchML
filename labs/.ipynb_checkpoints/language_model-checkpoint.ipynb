{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d182ebe-b91c-4d96-becf-d5862d6b07d0",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general.\n",
    "\n",
    "The main generative model we use throughout the book is Phi-3-mini, which is a relatively small (3.8 billion parameters) but quite performant model.16 Due to its small size, the model can be run on devices with less than 8 GB of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a468bdc-dabb-4474-bbda-65738f565570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "#!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc420f-eb47-4cb6-822a-7641c21a55a7",
   "metadata": {},
   "source": [
    "When you use an LLM, two models are loaded: the generative model and the underlying tokenizer\n",
    "\n",
    "The tokenizer splits the input text into tokens before feeding it to the generative model. \n",
    "\n",
    "Use transformers to load both the tokenizer and model. Works best with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcdb08-37df-41b3-8548-5446c910dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4085b213bd8e4a728f5740d1ba3b6038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfef4ecec9c44c5b4911ea0e9a5c733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   6%|6         | 304M/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    # device_map=\"cuda\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04183b-4ed9-41b8-97b9-23914c055d0c",
   "metadata": {},
   "source": [
    "transformers.pipeline simplifie the process of generating text. It encapsulates the model, tokenizer, and text generation process into a single function:\n",
    "\n",
    "Relevant parameters:\n",
    "\n",
    "return_full_text - By setting this to False, the prompt will not be returned, only the output of the model.\n",
    "\n",
    "max_new_tokens - The maximum number of tokens the model will generate. By setting a limit, we prevent long and unwieldy output as some models might continue generating output until they reach their context window.\n",
    "\n",
    "do_sample - Whether the model uses a sampling strategy to choose the next token. By setting this to False, the model will always select the next most probable token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12adf14-5f4d-4f64-a107-7688f9163091",
   "metadata": {},
   "source": [
    "To generate our first text, instruct the model to tell a joke about chickens. \n",
    "\n",
    "Format the prompt in a list of dictionaries where each dictionary relates to an entity in the conversation. \n",
    "\n",
    "Our role is that of “user” and we use the “content” key to define our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76280e7-833e-427d-bf5d-880f5552d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd945b-33a2-424c-a3b4-66768b87ea90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "options_dashboard",
   "language": "python",
   "name": "options_dashboard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
