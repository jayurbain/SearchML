{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmjqz349-e-W"
      },
      "source": [
        "PinSage Graph Recommendation using ogbn.\n",
        "Jay Urbain, PhD   \n",
        "5/30/2023\n",
        "\n",
        "References:   \n",
        "\n",
        "Building product graphs automatially, Amazon.     \n",
        "https://www.amazon.science/blog/building-product-graphs-automatically\n",
        "\n",
        "Amazon Product co-purchasing networks dataset:    \n",
        "http://snap.stanford.edu/data/index.html#amazon  \n",
        "\n",
        "GraphSage:    \n",
        "https://arxiv.org/abs/1706.02216  \n",
        "\n",
        "ogbn-products:\n",
        "- Undirecrted, unweighted graph representing product co-purchasing network to\n",
        "predict shopping preferences.    \n",
        "https://ogb.stanford.edu/docs/nodeprop/#ogbn-products\n",
        "\n",
        "Open Benchmark Dataset:   \n",
        "https://ogb.stanford.edu/  \n",
        "\n",
        "Blog articles:   \n",
        "\n",
        "https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48\n",
        "\n",
        "https://towardsdatascience.com/a-comprehensive-case-study-of-graphsage-algorithm-with-hands-on-experience-using-pytorchgeometric-6fc631ab1067.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpxNvCrQ9LeM",
        "outputId": "28901075-7182-4ef8-8074-fee313107509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3620314 sha256=52ef4de154ab72c29eeef6f64391f504d898f9553d68896aae626bca8302b9a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2813759 sha256=093c5bf95196ea7a5729b6de29c29e6f630826e40caaade94fffe0d8af88a914\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2057059 sha256=3892764f834cbdd1b772f41dcf56296fa52003ba052463bf0eebad4f1cb42741\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-spline-conv\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=567901 sha256=0f294d9b208356a25e51c1ce225489b21ec86280c7e345f1a82b4d814de20281\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\n",
            "Successfully built torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# !nvidia-smi\n",
        "\n",
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA-EGkh3-ajS",
        "outputId": "a6c84de2-226d-4457-f36a-fe41d3b4ac7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1As_aaHEG7OH",
        "outputId": "30d1574e-646b-4085-b2d0-72dee599bd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis) (4.0.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, pyvis\n",
            "Successfully installed jedi-0.19.2 pyvis-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNE_8_k4JnWc"
      },
      "source": [
        "Dataset  \n",
        "\n",
        "Network collected by crawling Amazon website.  If a product is frequently co-purchased with product j, the graph contains a directed edge from i to j.\n",
        "It is based on Customers Who Bought This Item Also Bought feature of the Amazon website.\n",
        "The data was collected in March 02 2003.\n",
        "The graph has 262,111 nodes and 1,234,877 edges. Basic format of the data:\n",
        "\n",
        "```\n",
        "# Directed graph (each unordered pair of nodes is saved once): Amazon0302.txt\n",
        "# Amazon product co-purchaisng network from March 02 2003\n",
        "# Nodes: 262111 Edges: 1234877\n",
        "# FromNodeId\tToNodeId\n",
        "0\t1\n",
        "0\t2\n",
        "0\t3\n",
        "0\t4\n",
        "0\t5\n",
        "1\t0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vAhf-HHHpRC"
      },
      "source": [
        "## Building the Graph\n",
        "\n",
        "\n",
        "Treat each product as a node. Have an edge between any pair of products that are frequently bought together. Called a homogenous graph i.e. a graph where each node represents one type of entity and each edge represents one type of relationship.\n",
        "\n",
        "Can make this more complex by adding more information. For example: add users as nodes to the graph and add edges between users and the products that they have bought. This would make the graph heterogenous i.e. a graph which can have nodes of multiple types and similarly edges of multiple types which can represent a variety of relationships between the nodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJgHjRp_KEtB"
      },
      "source": [
        "### Loading the data into a PyTorch Geometric Graph\n",
        "\n",
        "Need to convert data into a format which can be processed easily by PyG.\n",
        "\n",
        "Read all the lines in the file, initialize a numpy array and a list to keep track of the in-degree of each node and the edges respectively.\n",
        "\n",
        "Then all the lines are read one by one and processed: the lines with metadata are ignored and the lines with the start node and end node are processed. The in-degree of the end node is incremented and the edge data is added to edge_index.\n",
        "\n",
        "Use the in-degree of each node and the edge_index to create a PyG graph using the Data class.\n",
        "\n",
        "The graph is then saved using torch.save and optionally uploaded as a W&B artifact\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "kR4UtNFCCHD3",
        "outputId": "9fda845e-a885-41a4-d0c5-eada19c24ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjayurbain\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_175336-hkgneavo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jayurbain/gnn-recommender/runs/hkgneavo' target=\"_blank\">woven-salad-15</a></strong> to <a href='https://wandb.ai/jayurbain/gnn-recommender' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jayurbain/gnn-recommender' target=\"_blank\">https://wandb.ai/jayurbain/gnn-recommender</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jayurbain/gnn-recommender/runs/hkgneavo' target=\"_blank\">https://wandb.ai/jayurbain/gnn-recommender/runs/hkgneavo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-salad-15</strong> at: <a href='https://wandb.ai/jayurbain/gnn-recommender/runs/hkgneavo' target=\"_blank\">https://wandb.ai/jayurbain/gnn-recommender/runs/hkgneavo</a><br> View project at: <a href='https://wandb.ai/jayurbain/gnn-recommender' target=\"_blank\">https://wandb.ai/jayurbain/gnn-recommender</a><br>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_175336-hkgneavo/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"gnn-recommender\", job_type=\"preprocessing\", save_code=True)\n",
        "file_path = wandb.use_artifact(\"manan-goel/gnn-recommender/raw-data:v0\").download()\n",
        "with open(f'{file_path}/amazon0302.txt', 'r') as f:\n",
        "    edges = f.readlines()\n",
        "\n",
        "    idx = 0\n",
        "    edge_index = []\n",
        "    in_degrees = np.zeros((262111, 1))\n",
        "\n",
        "    while idx < len(edges):\n",
        "        print(f\"{idx}/{len(edges)}\", end='\\r')\n",
        "        line = edges[idx]\n",
        "        if line.startswith('#'):\n",
        "            idx += 1\n",
        "            continue\n",
        "        start, end = line.strip().split()\n",
        "        start, end = int(start), int(end)\n",
        "        in_degrees[end][0] += 1\n",
        "\n",
        "        edge_index.append([start, end])\n",
        "        idx += 1\n",
        "\n",
        "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
        "    graph = Data(x=in_degrees, edge_index=edge_index)\n",
        "\n",
        "    graph_artifact = wandb.Artifact('amazon_product_graph', type='graph')\n",
        "\n",
        "    torch.save(graph, 'amazon0302.pt')\n",
        "\n",
        "    graph_artifact.add_file('amazon0302.pt')\n",
        "    wandb.log_artifact(graph_artifact)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdduA8QGS5h"
      },
      "source": [
        "### Visualizing the PyTorch Geometric Graph\n",
        "\n",
        "Hard to visualize hundreds of thousands of nodes so sample the first 100 nodes from the graph using the subgraph utility from PyTorch Geometric.\n",
        "\n",
        "Use the metadata of the nodes available as a part of the dataset for creating a more informative visualization.\n",
        "\n",
        "This is very slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "4d95ee43ea594cf1914ce04d302ee1c6",
            "55e557365a7b49d2b8c57adc3a2a1157",
            "14eefe18719642818c27376dad3af0e2",
            "0a0e9e2d62024c79a73acf0e0b057376",
            "1fb4e2112f854f5ca1f011a0e5115ecb",
            "721e593b88fe43ceb0e29ed9470baaf6",
            "6441318cded64566b08d929baaa55b75",
            "7ae237d70ae14049a65c164ecd3ef384",
            "81cd938714d4438b91b72d73f87929b5",
            "8a9cc3f2b968481c9912d8048656fe67",
            "2566353a7e354dada842a20c3d408039"
          ]
        },
        "id": "6hBSqtjaPKYc",
        "outputId": "d08e852a-b80e-4afe-9dba-58995212d48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact processed_metadata:latest, 71.56MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/218424 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d95ee43ea594cf1914ce04d302ee1c6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyvis.network import Network\n",
        "from torch_geometric import utils\n",
        "from torch_geometric.data import Data\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "# Download the parsed metadata and smaller graph\n",
        "wandb.init(project=\"gnn-recommender\", job_type=\"eda\", save_code=True)\n",
        "metadata_path = wandb.use_artifact(\"manan-goel/gnn-recommender/processed_metadata:latest\").download()\n",
        "graph_path = wandb.use_artifact(\"manan-goel/gnn-recommender/smaller_graph:latest\").download()\n",
        "\n",
        "g = torch.load(f'{graph_path}/smaller_graph.pt', weights_only=False)\n",
        "\n",
        "with open(f'{metadata_path}/metadata.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "\n",
        "# Initialize the PyVis network\n",
        "net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
        "\n",
        "# Add the edges from the PyG graph to the PyVis network\n",
        "for e in tqdm(g.edge_index.T):\n",
        "\n",
        "    src = e[0].item()\n",
        "    dst = e[1].item()\n",
        "    if src == 0 or dst == 0:\n",
        "        continue\n",
        "    src_title = \"Title:\" + metadata[src]['title'] + \"\\n\\n\" + \"Categories:\\n\" + \"\\n\".join(list(metadata[src]['categories'])[:3])\n",
        "    dst_title = \"Title:\" + metadata[dst]['title'] + \"\\n\\n\" + \"Categories:\\n\" + \"\\n\".join(list(metadata[dst]['categories'])[:3])\n",
        "    net.add_node(dst, label=src_title, title=src_title)\n",
        "    net.add_node(src, label=dst_title, title=dst_title)\n",
        "    net.add_edge(src, dst, value=0.1)\n",
        "\n",
        "# Save the PyVis visualisation to a HTML file\n",
        "net.show(\"graph.html\")\n",
        "\n",
        "# Log the interactive HTML to W&B\n",
        "wandb.log({\"eda/graph\": wandb.Html(\"graph.html\")})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIyBH9y0Hu57"
      },
      "source": [
        "### Creating Node Features\n",
        "\n",
        "For every node in the graph, provide the model with some information about what the node represents.\n",
        "\n",
        "A basic way of doing this is using the in-degree of the node as a feature. But using in-degree does not add any information about what the product represents so it may not lead to good performance.\n",
        "\n",
        "So it's important to \"featurize\" the nodes in the most robust way possible. For example, use Doc2Vec embeddings for the product titles of each node and use the embedding as the input node feature.\n",
        "\n",
        "Another possible option is to use metadata or a list of categories to which each product belongs.\n",
        "\n",
        "### The Link Prediction Problem\n",
        "\n",
        "One of the fundamental problems in geometric deep learning is link prediction, i.e. predicting whether an edge exists between two nodes in a graph.\n",
        "\n",
        "This can be used to predict whether two users can be friends in a social network,  predicting interactions between genes and proteins in a biological network, or how similar or related two products are.\n",
        "\n",
        "Basic idea is to see if two products are similar and hence, can be recommended as suggestions when someone is looking at one of them or not.\n",
        "\n",
        "Basic way to measure similarity is using graph embeddings. Graph embedding algorithms learn an embedding space in which neighboring nodes are represented by vectors so that vector similarity measures, such as dot product similarity, or euclidean distance, hold in the embedding space.\n",
        "\n",
        "These can be learned using graph convolutional neural networks (GCNs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqYMOCWDh2il"
      },
      "source": [
        "### Link Prediction\n",
        "\n",
        "#### Splitting the Dataset\n",
        "\n",
        "Create a train, test and validation split of the edges in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yGRivTnDh0-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "3c8d2b0c-93df-43d8-e8f8-9ca86e653281"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'smaller_graph.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fa6692b04fdc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gnn-recommender\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"manan-goel/gnn-recommender/smaller_graph:latest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smaller_graph.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'smaller_graph.pt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "import wandb\n",
        "\n",
        "\n",
        "# Download and load the graph from W&B artifacts\n",
        "wandb.init(project=\"gnn-recommender\", job_type=\"preprocessing\", save_code=True)\n",
        "wandb.use_artifact(\"manan-goel/gnn-recommender/smaller_graph:latest\")\n",
        "graph = torch.load('smaller_graph.pt')\n",
        "\n",
        "\n",
        "# Add 5000 edges in the validation and test sets respectively\n",
        "transform = RandomLinkSplit(num_val=5000, num_test=5000, is_undirected=True, split_labels=True)\n",
        "train_data, val_data, test_data = transform(graph)\n",
        "\n",
        "\n",
        "# Save the splits and save as W&B artifacts\n",
        "torch.save(train_data, 'train.pt')\n",
        "torch.save(val_data, 'val.pt')\n",
        "torch.save(test_data, 'test.pt')\n",
        "\n",
        "\n",
        "artifact = wandb.Artifact('split_smaller_graph', type='graph')\n",
        "artifact.add_file('train.pt')\n",
        "artifact.add_file('val.pt')\n",
        "artifact.add_file('test.pt')\n",
        "\n",
        "\n",
        "wandb.log_artifact(artifact)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_52oA4LRiKss"
      },
      "source": [
        "Implementing the Model consists of two parts: arriving at a node embedding using graph convolutions for two nodes followed by using the two embeddings to make a final prediction whether a link exists or not.\n",
        "\n",
        "Graph Convolution\n",
        "There are multiple kinds of graph convolution models available in PyTorch Geometric. Using GraphSAGE model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx8scuyFiLC9"
      },
      "outputs": [],
      "source": [
        "import torch_geometric as pyg\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = pyg.nn.SAGEConv\n",
        "\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "\n",
        "        # Create num_layers GraphSAGE convs\n",
        "        assert (self.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(self.num_layers - 1):\n",
        "            self.convs.append(conv_model(hidden_dim, hidden_dim))\n",
        "\n",
        "\n",
        "        # post-message-passing processing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "\n",
        "        # Return final layer of embeddings if specified\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9dngUsVigsw"
      },
      "source": [
        "Link Prediction\n",
        "\n",
        "The previous module provides an embedding for a pair of nodes. This module is responsible for combining the two embeddings and making a binary prediction.\n",
        "\n",
        "This module is extremely similar to a full connected neural network with multiple linear layers stacked one after another.\n",
        "\n",
        "Forward Pass\n",
        "\n",
        "Take the embeddings for the two nodes as arguments and performs an element-wise product of the two. The obtained feature is then propagated through the linear layers and finally the sigmoid activation function is applied to find the final probability of their being an edge between the two nodes or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xn3tRqOihZ7"
      },
      "outputs": [],
      "source": [
        "class LinkPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        # Create linear layers\n",
        "        self.lins = nn.ModuleList()\n",
        "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        # x_i and x_j are both of shape (E, D)\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdy1p8RiuVU"
      },
      "source": [
        "Training the Model\n",
        "\n",
        "Training a link prediction model brings up a very interesting problem: the dataset we possess is a list of edges in the graph and when you think about it as a binary classification problem, this means we only have positive samples. Hence, there exists a concept called 'negative edges' i.e. edges that do not actually exist in the graph which we consider as negative samples. PyTorch Geometric provides a utility for this as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dOM3GGIiv5z"
      },
      "outputs": [],
      "source": [
        "def train(model, link_predictor, emb, edge_index, pos_train_edge, batch_size, optimizer):\n",
        "    model.train()\n",
        "    link_predictor.train()\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for edge_id in tqdm(pyg.loader.DataLoader(range(pos_train_edge.shape[0]), batch_size, shuffle=True), leave=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run message passing on the inital node embeddings to get updated embeddings\n",
        "        node_emb = model(emb, edge_index)  # (N, d)\n",
        "\n",
        "        # Predict the class probabilities on the batch of positive edges using link_predictor\n",
        "        pos_edge = pos_train_edge[edge_id].T  # (2, B)\n",
        "        pos_pred = link_predictor(node_emb[pos_edge[0]], node_emb[pos_edge[1]])  # (B, )\n",
        "\n",
        "        # Sample negative edges (same number as number of positive edges) and predict class probabilities\n",
        "        neg_edge = negative_sampling(edge_index, num_nodes=emb.shape[0],\n",
        "                                     num_neg_samples=edge_id.shape[0], method='dense')  # (Ne,2)\n",
        "        neg_pred = link_predictor(node_emb[neg_edge[0]], node_emb[neg_edge[1]])  # (Ne,)\n",
        "\n",
        "        # Compute the corresponding negative log likelihood loss on the positive and negative edges\n",
        "        loss = -torch.log(pos_pred + 1e-15).mean() - torch.log(1 - neg_pred + 1e-15).mean()\n",
        "\n",
        "        # Backpropagate and update parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "    return sum(train_losses) / len(train_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfF0KS3Ki5QF"
      },
      "outputs": [],
      "source": [
        "To finally initialize all the modules and train the model, you can use the following snippet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbw3LNwljAXy"
      },
      "outputs": [],
      "source": [
        "train_graph = torch.load('train.pt')\n",
        "val_graph = torch.load('val.pt')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optim_wd = 0\n",
        "epochs = 300\n",
        "hidden_dim = 1024\n",
        "dropout = 0.3\n",
        "num_layers = 2\n",
        "lr = 1e-5\n",
        "node_emb_dim = 1\n",
        "batch_size = 1024\n",
        "\n",
        "\n",
        "train_graph = train_graph.to(device)\n",
        "val_graph = val_graph.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = GNNStack(node_emb_dim, hidden_dim, hidden_dim, num_layers, dropout, emb=True).to(device) # the graph neural network that takes all the node embeddings as inputs to message pass and agregate\n",
        "link_predictor = LinkPredictor(hidden_dim, hidden_dim, 1, num_layers + 1, dropout).to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(link_predictor.parameters()),\n",
        "    lr=lr, weight_decay=optim_wd\n",
        ")\n",
        "\n",
        "\n",
        "train_loss = train(\n",
        "\tmodel,\n",
        "\tlink_predictor,\n",
        "\ttorch.tensor(train_graph.x).float().to(device),\n",
        "\ttrain_graph.edge_index,\n",
        "\ttrain_graph.pos_edge_label_index.T,\n",
        "\tbatch_size,\n",
        "\toptimizer\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTCPRdH3jKXK"
      },
      "source": [
        "Validating Model Performance\n",
        "One of the best metrics to validate model performance is measuring Hits@K which is the count of how many positive triples are ranked in the top-n positions against a bunch of synthetic negativ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR1dD6mLjL6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWBpnpzFjRHu"
      },
      "source": [
        "Testing\n",
        "For testing the performance of the model, we took 5,000 nodes from the product graph that had not been seen so far.  Initially we took 50,000 nodes from the product graph and for testing we took nodes that were not a part of this set of nodes. To test it, we obtained the learned embeddings of all the nodes in the test set and for 10 of them we saw which other nodes they were connected to using our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmOefH2bjRoJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d95ee43ea594cf1914ce04d302ee1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55e557365a7b49d2b8c57adc3a2a1157",
              "IPY_MODEL_14eefe18719642818c27376dad3af0e2",
              "IPY_MODEL_0a0e9e2d62024c79a73acf0e0b057376"
            ],
            "layout": "IPY_MODEL_1fb4e2112f854f5ca1f011a0e5115ecb"
          }
        },
        "55e557365a7b49d2b8c57adc3a2a1157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_721e593b88fe43ceb0e29ed9470baaf6",
            "placeholder": "​",
            "style": "IPY_MODEL_6441318cded64566b08d929baaa55b75",
            "value": " 74%"
          }
        },
        "14eefe18719642818c27376dad3af0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae237d70ae14049a65c164ecd3ef384",
            "max": 218424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81cd938714d4438b91b72d73f87929b5",
            "value": 161080
          }
        },
        "0a0e9e2d62024c79a73acf0e0b057376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9cc3f2b968481c9912d8048656fe67",
            "placeholder": "​",
            "style": "IPY_MODEL_2566353a7e354dada842a20c3d408039",
            "value": " 161080/218424 [34:42&lt;27:49, 34.34it/s]"
          }
        },
        "1fb4e2112f854f5ca1f011a0e5115ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721e593b88fe43ceb0e29ed9470baaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6441318cded64566b08d929baaa55b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ae237d70ae14049a65c164ecd3ef384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81cd938714d4438b91b72d73f87929b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a9cc3f2b968481c9912d8048656fe67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2566353a7e354dada842a20c3d408039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}